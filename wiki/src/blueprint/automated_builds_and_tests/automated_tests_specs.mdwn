[[!meta title="Automated tests specification"]]

*Ticket*: [[!tails_ticket 8667]]

This blueprint helps to keep track of the discussions and decisions
about the specification of automated tests ran in Tails' Jenkins on
the [[automatically build ISOs|autobuild_specs]] ([[!tails_ticket 5288]]).

[[!toc levels=2]]

# Facts

Running the full test suite on 1 isotester hosted on Lizard takes
around 8 hours.
We intend to run __4 isotesters__, so at the moment we would be able
to run __12 full test suites__ per day.

We have 2 isobuilders on Lizard, that build a total of a bit less than
__400 ISOs/month__ (that's __an average of 13 ISOs/day__).
At the moment, we build __from 6 ISOs to 30-40 ISOs a day__, depending
on the activity.

We usually build the _stable_, _devel_, _experimental_,
_feature/jessie_ (+ _testing_ sometimes) and a bunch of other
branches.

This numbers are expected to grow when the automated builds will be
put in production. It's difficult to guess what would the maximum
number of builds per day be, but the minimum should be expected to
raise to in between __10/20 ISOs/day__.

# Things to keep in mind

There will probably be new hardware at the end of the year to deploy
more isotesters. If a machine is dedicated to that usage, we can throw
in faster CPUs and run the test suite on bare metal, which would
speed up the test process. That's [[!tails_ticket 9264]].

So in this discussion, we have to think to a deployment that might
have two iterations with different computational powers (and thus
different amounts of tests/day possible), and the defined
implementation should be modular enough to handle both of them without
too many changes.

# Questions

<a id="when-to-test-the-builds"></a>

## When to test the builds

It doesn't sound reasonable to run the full test suite on every ISO
automatically built by Jenkins, at least in the first iteration.

We __need to lessen the number of tests per day__. Probably even in
the second iteration, with the expected growth of the number of
automated builds.

Some ideas that could help:

We can maybe split the design between the type of branches being built
and tested:

* for base branches, we could envisage to run the full test suite on
  every automatically built ISO (every git push and daily builds) if
  we think that is relevant;
* for feature branches, we could run the full test suite only on the
  daily builds, and either only the automated tests related to the
  branch on every git push, and/or a subset of the whole test suite.

We can also consider testing only the feature branches that are marked
as *Ready for QA* as a beginning, even if that doesn't cover Scenario 2
(developers).

We can also maybe find more ways to split the automated test suite in
faster subsets of features depending on the context, define priorities
for built ISO and/or tests.

<a id="how-to-run-the-tests"></a>

## How to run the tests

This section heavily depends on the discussion about the previous one.

The automated test suite MUST have access to the artifacts of a given
automated build corresponding to a given commit, as well as to the
ISOs of the previous Tails releases.

The automated test suite MUST be run in a clean environment.

The automated test suite MUST be run on a freshly built ISO,
corresponding to the commit it tests.

The automated test suite MUST be able to run features in parallel
for a single automated build ISO. This way, if more than one isotester
are idle, it can use several of them to test an ISO faster.

The automated suite SHOULD be able when there are more than one ISO
queued for testing to fairly distribute the parallelizing of their
features.

The automated test suite MUST not allocate all the isotesters for one
ISO when others are waiting to be tested.

The automated test suite MUST be able to accept a treshold of failures
for some features before sending notifications. This can help if a
scenario fails because of a network congestion, but other use cases
will probably raise.

## Notifications

Email will be the main interface. Use the same settings than for
automated builds:

* For base branches, notify the RM.
* For topic branches, notify the author of the last commit.

# Scenarios

In the following scenario:

0. topic branches are named branch F
0. base branches are named branch B

We also assume that the following scenarios expectations only cover
the test suite artifacts, the ISO one being covered in the automated
builds specification.

## Scenario 1 : reviewer

    As a reviewer
    When I'm asked to review branch F into branch B
    Then I need to know if branch F passes the automated tests
      once locally merged into branch B and built (fresh results!)
    And if all the automated tests scenarios suceeded
      The resulting test logs must be made available to me
      The Redmine ticket should be notified
    Otherwise the developer who proposed the merge must be notified
      And the developer *needs* to see the test logs and screenshots
      And the ticket should be reassigned to the branch submitter
      And QA check should be set to "Dev needed"

## Scenario 2 : developer

    As a developer who has the commit bit
    When I'm working on branch F
    Then I need to know if my branch passes the automated tests suite
      once locally merged in base branch B
    And I need to know if my branch fails to pass the automated test
      suite because of an external change possibly weeks after my
      last commit (by e.g Debian changes, changes in branch B, ...)
    And if my branch passes the automates test suite
      The resulting test logs must be made available to me
      The Redmine ticket should be notified
    Otherwise I *need* to see the build logs and screenshots
      And the developer who proposed the merge must be notified
      And the ticket should be reassigned to the branch submitter
      And QA check should be set to "Dev needed"


## Scenario 3 : RM

    As the current RM
    When working the full dev release cycle
    Then I need to know if a base branch does not pass the automated
      test suite
    And when this happens the build logs and screenshots must be made
      available to me.


# Future ideas

This list other scenarios that we have also envisaged for the second
iteration of the automated builds deployment, as they are really
tightened.

## Scenario 10

    As a Tails developer working on branch B
    When I upload a package to APT suite B
    Then I want to know if the automated test suite passes

(same responsiveness as when pushing to git)
(acceptable workaround: being able to manually trigger a test suite.)


## Scenario 11

    As the current RM
    When I push new tag T on branch B
    Then I want the APT suite for tag T to be created
    And I want the APT suite B to be copied into the APT suite T
    And once this is done, I want the automated test suite to be run
      on the ISO build from the checkout of tag T


## Scenario 12

    As a Tails developer
    When the test suite is ran on the ISO build from my last commit
    I want to watch TV and see the test video in HTML5 from the Tor Browser


## Scenario 14

    As a Tails developer
    When I push a new commit or a new Debian package to a base branch
    I want the affected feature branches ISO to be tested with that change
